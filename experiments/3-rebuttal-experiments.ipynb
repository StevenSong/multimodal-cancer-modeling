{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Survival Experiments\n",
    "\n",
    "This notebook contains all our code for survival modeling.\n",
    "\n",
    "The experiments test multimodal fusion of survival models and varying dimensionality reductions for high-dimensional embeddings.\n",
    "\n",
    "Specifically, we experiment with 5 modalities:\n",
    "* Patient demographics (sex, age - binned, race, ethnicity)\n",
    "* Cancer type (we use the TCGA project ID as a proxy for cancer type)\n",
    "* RNA-seq gene expression (`BulkRNABert` embeddings)\n",
    "* Whole slide histology images (`UNI2` embeddings)\n",
    "* Pathology reports (`BioMistral` embeddings)\n",
    "\n",
    "We additionally experiment with various alternate embeddings, including:\n",
    "* `BioMistral` embeddings of pathology report summaries generated by `Llama-3.1-8B-Instruct`\n",
    "* `Mistral-7B-Instruct-v0.1` embeddings of pathology reports\n",
    "* `Mistral-7B-Instruct-v0.1` embeddings of pathology report summaries generated by `Llama-3.1-8B-Instruct`\n",
    "* `UCE` embeddings of RNA-seq gene expression\n",
    "\n",
    "To use these alternate embeddings, modify the variables for input/output files in the first code cell of this notebook.\n",
    "\n",
    "Run experiments by executing all cells of this notebook. Results are saved in the `results` subdirectory at the root of the repo. Analysis and visualization is done using tools also in the `results` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_file = \"../embed/expr.h5\" # BulkRNABert\n",
    "hist_file = \"../embed/hist.h5\" # UNI2\n",
    "text_file = \"../embed/summ.h5\" # BioMistral - Summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pqdm.processes import pqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored, integrated_brier_score, cumulative_dynamic_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clinical.csv\")\n",
    "clin_case_ids = set(df[\"case_id\"])\n",
    "\n",
    "with h5py.File(expr_file, \"r\") as expr_h5:\n",
    "    expr_case_ids = set(expr_h5.keys())\n",
    "\n",
    "with h5py.File(hist_file, \"r\") as hist_h5:\n",
    "    hist_case_ids = set(hist_h5.keys())\n",
    "\n",
    "with h5py.File(text_file, \"r\") as text_h5:\n",
    "    text_case_ids = set(text_h5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[\"days_to_death\"].where(df[\"days_to_death\"].notna(), df[\"days_to_last_follow_up\"]) <= 365).sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[\"days_to_death\"].where(df[\"days_to_death\"].notna(), df[\"days_to_last_follow_up\"]) <= 5*365).sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ids = sorted(list(clin_case_ids & expr_case_ids & hist_case_ids & text_case_ids))\n",
    "\n",
    "df = df[df[\"case_id\"].isin(case_ids)]\n",
    "df = df.sort_values(\"case_id\").reset_index(drop=True)\n",
    "assert df[\"case_id\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age_binned\"] = pd.cut(\n",
    "    df[\"age\"],\n",
    "    bins=[0, 20, 40, 60, 80, 100],\n",
    "    labels=[\"(0, 20]\", \"(20, 40]\", \"(40, 60]\", \"(60, 80]\", \"(80, 100]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dead = df[\"vital_status\"] == \"Dead\"\n",
    "days_to_event = np.where(dead, df[\"days_to_death\"], df[\"days_to_last_follow_up\"])\n",
    "assert not np.isnan(days_to_event).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(zip(dead, days_to_event)), dtype=[('Status', '?'), ('Survival_in_days', '<f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32)\n",
    "canc_ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_X = demo_ohe.fit_transform(df[[\"sex\", \"age_binned\", \"race\", \"ethnicity\"]])\n",
    "canc_X = canc_ohe.fit_transform(df[[\"project\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "canc_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "canc_ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_case_emb_from_h5(case_ids: list[str], h5: h5py.File):\n",
    "    X = []\n",
    "    for case_id in tqdm(case_ids):\n",
    "        case_group = h5[case_id]\n",
    "        embs = np.stack([v[:] for v in case_group.values()], axis=0)\n",
    "        emb = np.mean(embs, axis=0)\n",
    "        X.append(emb)\n",
    "    return np.stack(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(expr_file, \"r\") as expr_h5:\n",
    "    expr_X = extract_case_emb_from_h5(case_ids, expr_h5)\n",
    "\n",
    "with h5py.File(hist_file, \"r\") as hist_h5:\n",
    "    hist_X = extract_case_emb_from_h5(case_ids, hist_h5)\n",
    "\n",
    "with h5py.File(text_file, \"r\") as text_h5:\n",
    "    text_X = extract_case_emb_from_h5(case_ids, text_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "splitter = (\n",
    "    df[\"vital_status\"]\n",
    "    + \"_\"\n",
    "    + df[\"project\"]\n",
    "    + \"_\"\n",
    "    + df[\"sex\"]\n",
    "    + \"_\"\n",
    "    + df[\"age_binned\"].astype(str)\n",
    "    + \"_\"\n",
    "    + df[\"race\"]\n",
    "    + \"_\"\n",
    "    + df[\"ethnicity\"]\n",
    ")\n",
    "\n",
    "n = len(df)\n",
    "test_splits = [split_idxs for _, split_idxs in skf.split(X=np.zeros(n), y=splitter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = df[[\"case_id\"]].copy()\n",
    "meta_df[\"split\"] = -1\n",
    "meta_df[\"split_order\"] = -1\n",
    "for i, test_idxs in enumerate(test_splits):\n",
    "    meta_df.loc[test_idxs, \"split\"] = i\n",
    "    meta_df.loc[test_idxs, \"split_order\"] = list(range(len(test_idxs)))\n",
    "meta_df[\"dead\"] = y[\"Status\"]\n",
    "meta_df[\"days_to_death_or_censor\"] = y[\"Survival_in_days\"]\n",
    "# meta_df.to_csv(\"../results/split_cases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split(\n",
    "    *,  # enforce kwargs\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    pca_components: int | None,\n",
    "    standardize: bool,\n",
    "    name: str = \"\",\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    if verbose:\n",
    "        print(f\"Running {name}\")\n",
    "\n",
    "    # z-score input features\n",
    "    if standardize:\n",
    "        if verbose:\n",
    "            print(\"--standardized\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "\n",
    "    # dimensionality reduction\n",
    "    if pca_components is not None:\n",
    "        if verbose:\n",
    "            print(\"--reduced\")\n",
    "        pca = PCA(n_components=pca_components, random_state=42)\n",
    "        X_train_red = pca.fit_transform(X_train_scaled)\n",
    "        X_test_red = pca.transform(X_test_scaled)\n",
    "    else:\n",
    "        X_train_red = X_train_scaled\n",
    "        X_test_red = X_test_scaled\n",
    "\n",
    "    # fit survival model\n",
    "    cox = CoxPHSurvivalAnalysis(alpha=0.1).fit(X_train_red, y_train)\n",
    "    if verbose:\n",
    "        print(\"--trained\")\n",
    "\n",
    "    # generate predictions\n",
    "    y_train_pred = cox.predict(X_train_red)\n",
    "    y_test_pred = cox.predict(X_test_red)\n",
    "\n",
    "    y_test_survs = cox.predict_survival_function(X_test_red)\n",
    "    times = np.arange(365, 1826) # 1 year to 5 year\n",
    "    y_test_probs = np.asarray([fn(times) for fn in y_test_survs])\n",
    "    if verbose:\n",
    "        print(\"--computed survival probabilities\")\n",
    "\n",
    "    # evaluate predictions\n",
    "    c_index = concordance_index_censored(\n",
    "        event_indicator=y_test[\"Status\"],\n",
    "        event_time=y_test[\"Survival_in_days\"],\n",
    "        estimate=y_test_pred,\n",
    "    )[0]\n",
    "    if verbose:\n",
    "        print(\"--computed c-index\")\n",
    "\n",
    "    ibs = integrated_brier_score(\n",
    "        survival_train=y_train,\n",
    "        survival_test=y_test,\n",
    "        estimate=y_test_probs,\n",
    "        times=times,\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"--computed IBS\")\n",
    "\n",
    "    _, cd_auc = cumulative_dynamic_auc(\n",
    "        survival_train=y_train,\n",
    "        survival_test=y_test,\n",
    "        estimate=y_test_pred,\n",
    "        times=times,\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"--computed cd-AUC\")\n",
    "\n",
    "    return {\n",
    "        \"c_index\": c_index,\n",
    "        \"ibs\": ibs,\n",
    "        \"cd_auc\": cd_auc,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"y_train_pred\": y_train_pred,\n",
    "        \"model\": cox,\n",
    "    }\n",
    "\n",
    "def run_unimodal_split(\n",
    "    *,  # enforce kwargs\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    test_idxs: np.ndarray,\n",
    "    train_idxs: np.ndarray,\n",
    "    pca_components: int | None,\n",
    "    standardize: bool,\n",
    "    name: str = \"\",\n",
    "    verbose: bool = False,\n",
    ") -> dict:\n",
    "    # split matrices\n",
    "    X_train, X_test = X[train_idxs], X[test_idxs]\n",
    "    y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "    return run_split(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        pca_components=pca_components,\n",
    "        standardize=standardize,\n",
    "        name=name,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "def powerset(s):\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(pca_components: int | None | dict[str, int | None]) -> dict:\n",
    "    if isinstance(pca_components, dict):\n",
    "        pca_map = pca_components\n",
    "    else:\n",
    "        pca_map = {\n",
    "            \"expr\": pca_components,\n",
    "            \"hist\": pca_components,\n",
    "            \"text\": pca_components,\n",
    "        }\n",
    "    results = []\n",
    "    for test_idxs in tqdm(test_splits, desc=\"Cross Validation Splits\"):\n",
    "        split_results = dict()\n",
    "\n",
    "        temp = set(test_idxs)\n",
    "        train_idxs = [i for i in range(n) if i not in temp]\n",
    "\n",
    "        split_results[\"demo\"] = run_unimodal_split(X=demo_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=None, standardize=False)\n",
    "        split_results[\"canc\"] = run_unimodal_split(X=canc_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=None, standardize=False)\n",
    "        split_results[\"expr\"] = run_unimodal_split(X=expr_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_map[\"expr\"], standardize=True)\n",
    "        split_results[\"hist\"] = run_unimodal_split(X=hist_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_map[\"hist\"], standardize=True)\n",
    "        split_results[\"text\"] = run_unimodal_split(X=text_X, y=y, test_idxs=test_idxs, train_idxs=train_idxs, pca_components=pca_map[\"text\"], standardize=True)\n",
    "\n",
    "        y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "        combos = [sorted(x) for x in powerset([\"demo\", \"canc\", \"expr\", \"hist\", \"text\"]) if len(x) > 1]\n",
    "        # combos = [sorted([\"demo\", \"canc\", \"expr\", \"hist\", \"text\"])]\n",
    "        for combo in combos:\n",
    "            mult_X_train = []\n",
    "            mult_X_test = []\n",
    "            for modality in combo:\n",
    "                x_train = split_results[modality][\"y_train_pred\"][:, np.newaxis]\n",
    "                x_test = split_results[modality][\"y_test_pred\"][:, np.newaxis]\n",
    "                # z-score all unimodal risks\n",
    "                scaler = StandardScaler()\n",
    "                x_train = scaler.fit_transform(x_train)\n",
    "                x_test = scaler.transform(x_test)\n",
    "                mult_X_train.append(x_train)\n",
    "                mult_X_test.append(x_test)\n",
    "\n",
    "            mult_X_train = np.concat(mult_X_train, axis=1)\n",
    "            mult_X_test = np.concat(mult_X_test, axis=1)\n",
    "\n",
    "            split_results[\"-\".join(combo)] = run_split(X_train=mult_X_train, y_train=y_train, X_test=mult_X_test, y_test=y_test, pca_components=None, standardize=False)\n",
    "\n",
    "        results.append(split_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Run mixed raw/reduced experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed = run_experiment(pca_components={\n",
    "    \"expr\": None,\n",
    "    \"hist\": None,\n",
    "    \"text\": 256,\n",
    "})\n",
    "np.save(\"../results/rebuttal_mixed_raw_embeddings_predictions_summarized.npy\", mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "combos = [\"-\".join(sorted(x)) for x in powerset([\"demo\", \"canc\", \"expr\", \"hist\", \"text\"]) if len(x) > 0]\n",
    "df = defaultdict(dict)\n",
    "for combo in combos:\n",
    "    temps = []\n",
    "    for i in range(5):\n",
    "        temp = mixed[i][combo][\"c_index\"]\n",
    "        temps.append(temp)\n",
    "    avg = np.mean(temps)\n",
    "    df[combo][\"Full Expr/Hist\"] = avg\n",
    "df = pd.DataFrame.from_dict(df, orient=\"index\")\n",
    "sorted_keys = sorted(sorted(combos), key=lambda x: len(x))\n",
    "df = df.loc[sorted_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(\"../results/results_summarized.csv\", index_col=0)\n",
    "orig = orig.rename(index={\"canc*\": \"canc\", \"demo*\": \"demo\", \"canc-demo*\": \"canc-demo\"})\n",
    "df.loc[df.index, \"PCA=256\"] = orig.loc[df.index, \"256\"]\n",
    "df.loc[[\"canc\", \"demo\", \"canc-demo\"], \"PCA=256\"] = orig.loc[[\"canc\", \"demo\", \"canc-demo\"], \"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[[\"canc\", \"demo\", \"expr\", \"hist\", \"text\", \"canc-demo-expr-hist-text\"], [\"PCA=256\", \"Full Expr/Hist\"]].to_markdown(floatfmt=\"0.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expr vs hist overfitting\n",
    "train_vs_test = defaultdict(dict)\n",
    "for mode_X, mode_name in [(expr_X, \"expr\"), (hist_X, \"hist\")]:\n",
    "    train_c_idxs = []\n",
    "    test_c_idxs = []\n",
    "    for i, test_idxs in enumerate(test_splits):\n",
    "        temp = set(test_idxs)\n",
    "        train_idxs = [i for i in range(n) if i not in temp]\n",
    "\n",
    "        X_train, X_test = mode_X[train_idxs], mode_X[test_idxs]\n",
    "        y_train, y_test = y[train_idxs], y[test_idxs]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        model = mixed[i][mode_name][\"model\"]\n",
    "        y_train_pred = model.predict(X_train_scaled)\n",
    "        y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        train_c_index = concordance_index_censored(\n",
    "            event_indicator=y_train[\"Status\"],\n",
    "            event_time=y_train[\"Survival_in_days\"],\n",
    "            estimate=y_train_pred,\n",
    "        )[0]\n",
    "        train_c_idxs.append(train_c_index)\n",
    "\n",
    "        test_c_index = concordance_index_censored(\n",
    "            event_indicator=y_test[\"Status\"],\n",
    "            event_time=y_test[\"Survival_in_days\"],\n",
    "            estimate=y_test_pred,\n",
    "        )[0]\n",
    "        test_c_idxs.append(test_c_index)\n",
    "\n",
    "    train_c_idxs = np.asarray(train_c_idxs)\n",
    "    test_c_idxs = np.asarray(test_c_idxs)\n",
    "\n",
    "    train_vs_test[mode_name][\"train\"] = train_c_idxs.mean()\n",
    "    train_vs_test[mode_name][\"test\"] = test_c_idxs.mean()\n",
    "\n",
    "train_vs_test = pd.DataFrame(train_vs_test).T\n",
    "print(train_vs_test.to_markdown(floatfmt=\"0.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Run the usual experiments with new metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for pca_components in tqdm([4, 8, 16, 32, 64, 128, 256]):\n",
    "    results[pca_components] = run_experiment(pca_components=pca_components)\n",
    "np.save(\"../results/rebuttal_predictions_summarized.npy\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_reduced = {\n",
    "    \"demo\",\n",
    "    \"canc\",\n",
    "    \"canc-demo\",\n",
    "}\n",
    "combos = [\"-\".join(sorted(x)) for x in powerset([\"demo\", \"canc\", \"expr\", \"hist\", \"text\"]) if len(x) > 0]\n",
    "metric_dfs = dict()\n",
    "for metric in [\"c_index\", \"ibs\", \"cd_auc\"]:\n",
    "    df = defaultdict(dict)\n",
    "    for pca_components in tqdm([4, 8, 16, 32, 64, 128, 256]):\n",
    "        for combo in combos:\n",
    "            temps = []\n",
    "            for i in range(5):\n",
    "                temp = results[pca_components][i][combo][metric]\n",
    "                temps.append(temp)\n",
    "            avg = np.mean(temps)\n",
    "            if combo in non_reduced:\n",
    "                if pca_components != 4:\n",
    "                    continue\n",
    "            df[combo][pca_components] = avg\n",
    "    df = pd.DataFrame.from_dict(df, orient=\"index\")\n",
    "    sorted_keys = sorted(sorted(combos), key=lambda x: len(x))\n",
    "    df = df.loc[sorted_keys]\n",
    "    df.columns.name = \"pca components\"\n",
    "    df.to_csv(f\"../results/rebuttal_{metric}_results_summarized.csv\")\n",
    "    metric_dfs[metric] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_dfs[\"c_index\"].loc[[\"canc\", \"demo\", \"expr\", \"hist\", \"text\", \"canc-demo-expr-hist-text\"]].to_markdown(floatfmt=\"0.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Extra Metrics (IBS and C/D-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_dfs[\"ibs\"].loc[[\"canc\", \"demo\", \"expr\", \"hist\", \"text\", \"canc-demo-expr-hist-text\"]].to_markdown(floatfmt=\"0.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric_dfs[\"cd_auc\"].loc[[\"canc\", \"demo\", \"expr\", \"hist\", \"text\", \"canc-demo-expr-hist-text\"]].to_markdown(floatfmt=\"0.3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Multimodal Model Hazard Ratios for Each Modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = []\n",
    "for i in range(5):\n",
    "    model = results[256][i][\"canc-demo-expr-hist-text\"][\"model\"]\n",
    "    coefs.append(model.coef_)\n",
    "coefs = np.asarray(coefs)\n",
    "coefs = pd.DataFrame(data=coefs, index=pd.Index(np.arange(5), name=\"Split\"), columns=[\"Canc\", \"Demo\", \"Expr\", \"Hist\", \"Text\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs[\"Mean\"] = coefs.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "HRs = np.exp(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HRs.to_markdown(floatfmt=\"0.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
